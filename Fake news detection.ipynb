{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4788e2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\anaconda\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f1c7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  target\n",
      "0      Donald Trump just couldn t wish all Americans ...       0\n",
      "1      House Intelligence Committee Chairman Devin Nu...       0\n",
      "2      On Friday, it was revealed that former Milwauk...       0\n",
      "3      On Christmas day, Donald Trump announced that ...       0\n",
      "4      Pope Francis used his annual Christmas Day mes...       0\n",
      "...                                                  ...     ...\n",
      "44893  BRUSSELS (Reuters) - NATO allies on Tuesday we...       1\n",
      "44894  LONDON (Reuters) - LexisNexis, a provider of l...       1\n",
      "44895  MINSK (Reuters) - In the shadow of disused Sov...       1\n",
      "44896  MOSCOW (Reuters) - Vatican Secretary of State ...       1\n",
      "44897  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...       1\n",
      "\n",
      "[44898 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fake = pd.read_csv('Fake.csv')\n",
    "true = pd.read_csv('True.csv')\n",
    "fake = fake.drop(['subject','date','title'],axis=1)\n",
    "true = true.drop(['subject','date','title'],axis=1)\n",
    "fake['target'] = 0\n",
    "true['target'] = 1\n",
    "data = pd.concat([fake,true],axis=0).reset_index(drop='True')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dcf1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\anaconda\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2777f0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Penukonda\n",
      "[nltk_data]     Chandana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0542539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  [Donald, Trump, just, couldn, t, wish, all, Am...       0\n",
      "1  [House, Intelligence, Committee, Chairman, Dev...       0\n",
      "2  [On, Friday, ,, it, was, revealed, that, forme...       0\n",
      "3  [On, Christmas, day, ,, Donald, Trump, announc...       0\n",
      "4  [Pope, Francis, used, his, annual, Christmas, ...       0\n",
      "5  [The, number, of, cases, of, cops, brutalizing...       0\n",
      "6  [Donald, Trump, spent, a, good, portion, of, h...       0\n",
      "7  [In, the, wake, of, yet, another, court, decis...       0\n",
      "8  [Many, people, have, raised, the, alarm, regar...       0\n",
      "9  [Just, when, you, might, have, thought, we, d,...       0\n"
     ]
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(word_tokenize)\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f014352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb45f548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  [donald, trump, just, couldn, t, wish, all, am...       0\n",
      "1  [hous, intellig, committe, chairman, devin, nu...       0\n",
      "2  [on, friday, ,, it, was, reveal, that, former,...       0\n",
      "3  [on, christma, day, ,, donald, trump, announc,...       0\n",
      "4  [pope, franci, use, his, annual, christma, day...       0\n",
      "5  [the, number, of, case, of, cop, brutal, and, ...       0\n",
      "6  [donald, trump, spent, a, good, portion, of, h...       0\n",
      "7  [in, the, wake, of, yet, anoth, court, decis, ...       0\n",
      "8  [mani, peopl, have, rais, the, alarm, regard, ...       0\n",
      "9  [just, when, you, might, have, thought, we, d,...       0\n"
     ]
    }
   ],
   "source": [
    "def stem_it(text):\n",
    "    return [stemmer.stem(word) for word in text]\n",
    "data['text'] = data['text'].apply(stem_it)\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33032c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  [donald, trump, just, couldn, wish, all, ameri...       0\n",
      "1  [hous, intellig, committe, chairman, devin, nu...       0\n",
      "2  [friday, was, reveal, that, former, milwauke, ...       0\n",
      "3  [christma, day, donald, trump, announc, that, ...       0\n",
      "4  [pope, franci, use, his, annual, christma, day...       0\n",
      "5  [the, number, case, cop, brutal, and, kill, pe...       0\n",
      "6  [donald, trump, spent, good, portion, his, day...       0\n",
      "7  [the, wake, yet, anoth, court, decis, that, de...       0\n",
      "8  [mani, peopl, have, rais, the, alarm, regard, ...       0\n",
      "9  [just, when, you, might, have, thought, get, b...       0\n"
     ]
    }
   ],
   "source": [
    "def stop_it(text):\n",
    "    return [word for word in text if len(word)>2]\n",
    "data['text'] = data['text'].apply(stop_it)\n",
    "print(data.head(10))\n",
    "data['text'] = data['text'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80418b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31911    reuter hacker target the comput system preside...\n",
      "1396     long befor presidenti scandal were serious con...\n",
      "27635    washington reuter presid donald trump said thu...\n",
      "30511    washington reuter republican antitrust veteran...\n",
      "13980    the globalist aren happi which signal that the...\n",
      "23935    washington/london reuter presid donald trump w...\n",
      "37937    warsaw reuter ukrain has summon the polish amb...\n",
      "35445    johannesburg reuter the near man south african...\n",
      "11981                                                     \n",
      "604      just took massiv step closer nuclear war with ...\n",
      "Name: text, dtype: object\n",
      "31911    1\n",
      "1396     0\n",
      "27635    1\n",
      "30511    1\n",
      "13980    0\n",
      "23935    1\n",
      "37937    1\n",
      "35445    1\n",
      "11981    0\n",
      "604      0\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'],data['target'],test_size=0.2)\n",
    "print(X_train.head(10))\n",
    "print(y_train.head(10))\n",
    "# print(X_test.head(10))\n",
    "# print(y_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d34e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 84137)\t0.04350813081742632\n",
      "  (0, 31086)\t0.07914835010447467\n",
      "  (0, 22568)\t0.0631979540925936\n",
      "  (0, 91508)\t0.023829423282051714\n",
      "  (0, 8539)\t0.02750744450814722\n",
      "  (0, 25483)\t0.04352503308953736\n",
      "  (0, 88417)\t0.08125077156368739\n",
      "  (0, 43574)\t0.07999483751102754\n",
      "  (0, 41835)\t0.05070015847341079\n",
      "  (0, 62138)\t0.042999303354822695\n",
      "  (0, 40645)\t0.058672510527133764\n",
      "  (0, 25953)\t0.03725956184235436\n",
      "  (0, 32220)\t0.048415073331327094\n",
      "  (0, 41831)\t0.06045468095866672\n",
      "  (0, 26900)\t0.06092607722147141\n",
      "  (0, 26560)\t0.029258181847707645\n",
      "  (0, 33833)\t0.040519978673506186\n",
      "  (0, 20847)\t0.07017077446012923\n",
      "  (0, 17491)\t0.09463746664510832\n",
      "  (0, 71431)\t0.02565412283860773\n",
      "  (0, 65248)\t0.052849688340100856\n",
      "  (0, 31407)\t0.057279441634577224\n",
      "  (0, 12023)\t0.04155093566703827\n",
      "  (0, 66640)\t0.08950372597977445\n",
      "  (0, 16252)\t0.06868338887433079\n",
      "  :\t:\n",
      "  (35917, 10500)\t0.031372396645010125\n",
      "  (35917, 53304)\t0.020130839874706905\n",
      "  (35917, 9611)\t0.0429524326376405\n",
      "  (35917, 91578)\t0.040458265432611926\n",
      "  (35917, 38910)\t0.047703327107576486\n",
      "  (35917, 88634)\t0.013299328623607647\n",
      "  (35917, 33641)\t0.0165108624357477\n",
      "  (35917, 67011)\t0.017370116781209415\n",
      "  (35917, 63399)\t0.033488435045953224\n",
      "  (35917, 90779)\t0.02629865431972961\n",
      "  (35917, 17771)\t0.017386119411751402\n",
      "  (35917, 15371)\t0.022018450030918655\n",
      "  (35917, 18211)\t0.024068504298124068\n",
      "  (35917, 50149)\t0.017458508611939184\n",
      "  (35917, 91325)\t0.021166740301768378\n",
      "  (35917, 49815)\t0.048586592483729284\n",
      "  (35917, 39060)\t0.015854652885695774\n",
      "  (35917, 57276)\t0.0286037462091175\n",
      "  (35917, 7794)\t0.0221572956475571\n",
      "  (35917, 61100)\t0.023795331165955044\n",
      "  (35917, 75706)\t0.029439016044337667\n",
      "  (35917, 63350)\t0.020929892905427183\n",
      "  (35917, 55350)\t0.015112241434682497\n",
      "  (35917, 79843)\t0.06275568476502266\n",
      "  (35917, 71832)\t0.009908026836629953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "my_tfidf = TfidfVectorizer(max_df=0.7)\n",
    "tfidf_train = my_tfidf.fit_transform(X_train)\n",
    "tfidf_test = my_tfidf.transform(X_test)\n",
    "print(tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1479f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a682828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.9086859688196\n"
     ]
    }
   ],
   "source": [
    "model_temp = LogisticRegression(max_iter=1000)\n",
    "model_temp.fit(tfidf_train,y_train)\n",
    "pred_temp = model_temp.predict(tfidf_test)\n",
    "acc_temp = accuracy_score(y_test,pred_temp)\n",
    "print(acc_temp*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "834dc1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.59910913140313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "model = PassiveAggressiveClassifier(max_iter=100)\n",
    "model.fit(tfidf_train,y_train)\n",
    "y_pred = model.predict(tfidf_test)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "print(acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d06ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas\n",
    "import pandas as pd\n",
    "fake = pd.read_csv('Fake.csv')\n",
    "true = pd.read_csv('True.csv')\n",
    "fake = fake.drop(['subject','date','title'],axis=1)\n",
    "true = true.drop(['subject','date','title'],axis=1)\n",
    "fake['target'] = 0\n",
    "true['target'] = 1\n",
    "data = pd.concat([fake,true],axis=0).reset_index(drop='True')\n",
    "print(data)\n",
    "pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "data['text'] = data['text'].apply(word_tokenize)\n",
    "print(data.head(10))\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "def stem_it(text):\n",
    "    return [stemmer.stem(word) for word in text]\n",
    "data['text'] = data['text'].apply(stem_it)\n",
    "print(data.head(10))\n",
    "def stop_it(text):\n",
    "    return [word for word in text if len(word)>2]\n",
    "data['text'] = data['text'].apply(stop_it)\n",
    "print(data.head(10))\n",
    "data['text'] = data['text'].apply(' '.join)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'],data['target'],test_size=0.2)\n",
    "print(X_train.head(10))\n",
    "print(y_train.head(10))\n",
    "# print(X_test.head(10))\n",
    "# print(y_test.head(10))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "my_tfidf = TfidfVectorizer(max_df=0.7)\n",
    "tfidf_train = my_tfidf.fit_transform(X_train)\n",
    "tfidf_test = my_tfidf.transform(X_test)\n",
    "print(tfidf_train)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model_temp = LogisticRegression(max_iter=1000)\n",
    "model_temp.fit(tfidf_train,y_train)\n",
    "pred_temp = model_temp.predict(tfidf_test)\n",
    "acc_temp = accuracy_score(y_test,pred_temp)\n",
    "print(acc_temp*100)\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "model = PassiveAggressiveClassifier(max_iter=100)\n",
    "model.fit(tfidf_train,y_train)\n",
    "y_pred = model.predict(tfidf_test)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "print(acc*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
